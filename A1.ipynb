{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tarfile\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class model:\n",
    "  def __init__(self, learning_rate):\n",
    "    self.index_dict = {} #dictionary of words:index in bag of words vector\n",
    "    #get reviews. add to init when done.\n",
    "    get_reviews('txt_sentoken/neg', 'txt_sentoken/pos')\n",
    "    #read_tar()\n",
    "    neg_path = 'txt_sentoken/neg'\n",
    "    pos_path = 'txt_sentoken/pos'\n",
    "    neg_files = os.listdir(neg_path)\n",
    "    pos_files = os.listdir(pos_path)\n",
    "\n",
    "    neg_rev, neg_sent, neg_words = self.get_data(neg_files, neg_path, -1)\n",
    "    pos_rev, pos_sent, pos_words = self.get_data(pos_files, pos_path, 1)\n",
    "    #form word bag, X_raw, and prediction sets\n",
    "    words = neg_words|pos_words\n",
    "    X_raw = np.concatenate((neg_rev,pos_rev),axis=0)\n",
    "    y = np.concatenate((neg_sent,pos_sent),axis=0)\n",
    "    sorted_words = sorted(words)\n",
    "    \n",
    "  def fit(self, X, y):\n",
    "    \"\"\"Does the training. X is a matrix with one data point per row, while y is flat.\"\"\"\n",
    "    X = X.ravel()\n",
    "    best_score = self.score(X,y)\n",
    "    self.loss.append(best_score)\n",
    "    early_stop,iter_wo_improve = 20,0\n",
    "    \n",
    "    for i in range(self.n_max_iterations):\n",
    "      self._fit(X,y)\n",
    "      if best_score == self.loss[-1]: #if the new theta isn't better than last, then increment iter_wo_improve.\n",
    "        iter_wo_improve += 1\n",
    "      else:\n",
    "        best_score = self.loss[-1]\n",
    "      if iter_wo_improve >= early_stop: #implement learning rate scheduler here\n",
    "        break\n",
    "  \n",
    "  def _fit(self, X, y): \n",
    "    \"\"\"Internal method that performs one iteration of the training. Should store a loss value in self.loss .\"\"\" \n",
    "    #is new param vector better than last one? compare score residual, if better keep, otherwise discard.\n",
    "    old_score = self.loss[-1] \n",
    "    old_theta = self.theta.copy()\n",
    "    self.theta += np.random.normal(size=len(self.theta)) #replace with gradient descent.\n",
    "    new_score = self.score(X,y)\n",
    "    if old_score > new_score:\n",
    "      self.loss.append(new_score)\n",
    "    else:\n",
    "      self.theta = old_theta\n",
    "\n",
    "  def predict(self, X): \n",
    "    \"\"\"Predicts outputs y from some inputs X\"\"\" \n",
    "    #pred = np.vstack(np.linspace(0, 0, num=len(X)))\n",
    "    pred = np.zeros(len(X))\n",
    "    for i in range(self.n_order+1):\n",
    "      pred += X.ravel()**i*self.theta[i]\n",
    "    return pred\n",
    "\n",
    "  #hinge loss\n",
    "  def score(self, X, y):\n",
    "    ...\n",
    "  \n",
    "  \"\"\"_summary_\n",
    "  transform x into a sparse BoW array. implementation part 2\n",
    "  \"\"\"\n",
    "  def transform(self, data):\n",
    "    sparse_data = []\n",
    "    for review in data:\n",
    "        review_list = review.split()            #replace with tokenizer if needed\n",
    "        sparse_review = np.zeros(len(review_list))\n",
    "        for word in review_list:\n",
    "            sparse_review[self.index_dict[word]] = 1\n",
    "    sparse_data.append(sparse_review)\n",
    "            \n",
    "    assert len(data) == len(sparse_data)        #is the output array the same length as input?\n",
    "    assert sum(sparse_data[0]) != 0             #do these sparse arrays contain anything?\n",
    "    \n",
    "    return np.array(sparse_data)\n",
    "  \n",
    "  \"\"\"_summary_\n",
    "  grab data from directory paths and lists\n",
    "  \"\"\"\n",
    "  def get_data(self, dir_list, dir_path, review_polarity):\n",
    "    words = set()\n",
    "    reviews = []\n",
    "    sentiment = []\n",
    "    \n",
    "    for file_name in dir_list:\n",
    "        if isinstance(file_name, str):\n",
    "            f = open(os.path.join(dir_path, file_name),'r')\n",
    "            review = f.read()\n",
    "            reviews.append(review)\n",
    "            words = words|set(review.split()) #.strip(\"\") for future comparison\n",
    "            sentiment.append(review_polarity)\n",
    "            f.close()\n",
    "    return np.array(reviews), np.array(sentiment), words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Part1\n",
    "**Implementation task:** Implement a parser for the dataset. The output should be a list/array of strings (`X_raw`) and a list/array of labels (`y`) encoded as {-1,1}."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -N http://www.cs.cornell.edu/people/pabo/movie-review-data/review_polarity.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_tar():\n",
    "    # open file \n",
    "    review_file = tarfile.open('review_polarity.tar.gz') \n",
    "  \n",
    "    # extracting file \n",
    "    review_file.extractall('.') \n",
    "  \n",
    "    review_file.close() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#shuffles both predictions and raw data with same permutation.\n",
    "def shuffle(data, labels):\n",
    "    p = np.random.permutation(len(data))\n",
    "    return data[p], labels[p]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_raw, y = shuffle(X_raw, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dimensions look good! now to move onto implementing BOW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Part2\n",
    "\n",
    "**Implementation task:** You should re-implement the feature extraction above. The list/array called `ordered_vocabulary` should contain the words for each feature dimension, and X should contain the BOW binary vectors. Remember to use the same method names as the original sklearn class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'dolphin' is represented as feature dimension 13868\n",
      "'the' is represented as feature dimension 45372\n",
      "'coffee' is represented as feature dimension 9677\n"
     ]
    }
   ],
   "source": [
    "index_dict = dict()\n",
    "for i, word in enumerate(sorted_words):\n",
    "  index_dict[word] = i\n",
    "\n",
    "for word in ['dolphin', 'the', 'coffee']:\n",
    "  if word in words:\n",
    "    print(\"'%s' is represented as feature dimension %i\" %(word, index_dict[word]))\n",
    "  else:\n",
    "    print(\"'%s' is not in the vocabulary\" % word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = set(sorted_words)      #I will take it as a given this will be useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "transform() missing 1 required positional argument: 'data'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_raw\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: transform() missing 1 required positional argument: 'data'"
     ]
    }
   ],
   "source": [
    "X = self.transform(X_raw)            #output should be bag of words rep of sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Part3\n",
    "**Implementation task:** You should implement your versions of the following parts (you can also find this in the slides):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "\"Set hyperparameters (these variables are only here for clarity)\"\n",
    "reguliser_dampening = 0.001   # lambda\n",
    "learning_rate = .1            # gamma\n",
    "\n",
    "\"Create the untrained classifier\"\n",
    "model = SGDClassifier(loss='hinge', penalty='l2',\n",
    "                      alpha=reguliser_dampening, verbose=1,\n",
    "                      learning_rate='constant', eta0=learning_rate)\n",
    "\n",
    "\"Train the classifier\"\n",
    "model.fit(X, y)\n",
    "\n",
    "\"Get the parameter vector\"\n",
    "omega = np.concatenate([model.intercept_, model.coef_.ravel()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Part4\n",
    "**Implementation task:** Implement code for printing a sorted table of your sampled hyperparameters. Note, you do not have to reimplement the grid search."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
