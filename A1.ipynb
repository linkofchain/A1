{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tarfile\n",
    "import jdc      #for class handling in jupyter\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class model:\n",
    "  def __init__(self, eta0 = 0.1, loss = 'hinge', alpha = .0001, learning_rate = \"constant\", penalty = 'L2', random_state = 'none',\n",
    "               tol = 5):\n",
    "    self.index_dict = {} #dictionary of words:index in bag of words vector\n",
    "    #read_tar()\n",
    "    neg_path = 'txt_sentoken/neg'\n",
    "    pos_path = 'txt_sentoken/pos'\n",
    "    neg_files = os.listdir(neg_path)\n",
    "    pos_files = os.listdir(pos_path)\n",
    "\n",
    "    neg_rev, neg_sent, neg_words = self.get_data(neg_files, neg_path, -1)\n",
    "    pos_rev, pos_sent, pos_words = self.get_data(pos_files, pos_path, 1)\n",
    "    #form word bag, X_raw, and prediction sets\n",
    "    words = neg_words|pos_words\n",
    "    self.X_raw = np.concatenate((neg_rev,pos_rev),axis=0)\n",
    "    self.y = np.concatenate((neg_sent,pos_sent),axis=0)\n",
    "    self.sorted_words = sorted(words)\n",
    "    \n",
    "    self.loss = loss  #hinge loss\n",
    "    self.loss_history = []\n",
    "    self.eta0 = eta0  #starting learning rate\n",
    "    self.alpha = alpha #regularization dampener\n",
    "    self.learning_rate = learning_rate  #do we use constant learning rate or schedule?\n",
    "    self.penalty = penalty #penalty for hinge loss i think?\n",
    "    self.random_state = random_state #random permutation desired for input? maybe not needed\n",
    "    self.tol = tol  #tolerance of learning rate scheduler if i ever get around to implementing that\n",
    "    self.weights = np.random.rand(self.X_raw.shape)\n",
    "    \n",
    "  def fit(self, X, y):\n",
    "    \"\"\"Does the training. X is a matrix with one data point per row, while y is flat.\"\"\"\n",
    "    X = X.ravel()\n",
    "    best_score = self.score(X,y)\n",
    "    self.loss.append(best_score)\n",
    "    early_stop,iter_wo_improve = 20,0\n",
    "    \n",
    "    for i in range(self.n_max_iterations):\n",
    "      self._fit(X,y)\n",
    "      if best_score == self.loss[-1]: #if the new theta isn't better than last, then increment iter_wo_improve.\n",
    "        iter_wo_improve += 1\n",
    "      else:\n",
    "        best_score = self.loss[-1]\n",
    "      if iter_wo_improve >= early_stop: #implement learning rate scheduler here\n",
    "        break\n",
    "      \n",
    "  #gradient descent here?\n",
    "  def _fit(self, X, y): \n",
    "    \"\"\"Internal method that performs one iteration of the training. Should store a loss value in self.loss .\"\"\" \n",
    "    #is new param vector better than last one? compare score residual, if better keep, otherwise discard.\n",
    "    old_score = self.loss[-1] \n",
    "    old_theta = self.theta.copy()\n",
    "    self.theta += np.random.normal(size=len(self.theta)) #replace with gradient descent.\n",
    "    new_score = self.score(X,y)\n",
    "    if old_score > new_score:\n",
    "      self.loss.append(new_score)\n",
    "    else:\n",
    "      self.theta = old_theta\n",
    "\n",
    "  #hyperplane model here?\n",
    "  def predict(self, X):\n",
    "    for review in X:\n",
    "      proddy = np.dot(review,self.weights)\n",
    "      if proddy < 0:\n",
    "        return -1\n",
    "      elif X == 0:\n",
    "        return 0\n",
    "      else:\n",
    "        return 1\n",
    "\n",
    "  #hinge loss for base\n",
    "  def score(self, X, y):\n",
    "    #how to define t? if on right side of hyperplane, 1, otherwise -1\n",
    "    if self.loss == 'hinge':\n",
    "      L2_sum = sum(max(0,1-))\n",
    "      \n",
    "      return max(0,1-t*y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part1\n",
    "**Implementation task:** Implement a parser for the dataset. The output should be a list/array of strings (`X_raw`) and a list/array of labels (`y`) encoded as {-1,1}."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!wget -N http://www.cs.cornell.edu/people/pabo/movie-review-data/review_polarity.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_tar():\n",
    "    # open file \n",
    "    review_file = tarfile.open('review_polarity.tar.gz') \n",
    "  \n",
    "    # extracting file \n",
    "    review_file.extractall('.') \n",
    "  \n",
    "    review_file.close() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"_summary_\n",
    "grab data from directory paths and lists\n",
    "\"\"\"\n",
    "%%add_to model\n",
    "def get_data(self, dir_list, dir_path, review_polarity):\n",
    "  words = set()\n",
    "  reviews = []\n",
    "  sentiment = []\n",
    "  \n",
    "  for file_name in dir_list:\n",
    "      if isinstance(file_name, str):\n",
    "          f = open(os.path.join(dir_path, file_name),'r')\n",
    "          review = f.read()\n",
    "          reviews.append(review)\n",
    "          words = words|set(review.split()) #.strip(\"\") for future comparison\n",
    "          sentiment.append(review_polarity)\n",
    "          f.close()\n",
    "  return np.array(reviews), np.array(sentiment), words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#shuffles both predictions and raw data with same permutation.\n",
    "%%add_to model\n",
    "    def shuffle(data, labels):\n",
    "        p = np.random.permutation(len(data))\n",
    "        return data[p], labels[p]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = model()\n",
    "classifier.X_raw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dimensions look good! now to move onto implementing BOW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part2\n",
    "\n",
    "**Implementation task:** You should re-implement the feature extraction above. The list/array called `ordered_vocabulary` should contain the words for each feature dimension, and X should contain the BOW binary vectors. Remember to use the same method names as the original sklearn class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"_summary_\n",
    "transform x into a sparse BoW array. implementation part 2\n",
    "\"\"\"\n",
    "class Vectorizer:\n",
    "  def __init__(self, tokenizer = 'none'):\n",
    "    self.tokenizer = tokenizer\n",
    "  \n",
    "  def transform(self, data):\n",
    "    sparse_data = []\n",
    "    for review in data:\n",
    "      review_list = review.split()            #replace with tokenizer if needed\n",
    "      sparse_review = np.zeros(len(review_list))\n",
    "      for word in review_list:\n",
    "          sparse_review[self.index_dict[word]] = 1\n",
    "    sparse_data.append(sparse_review)\n",
    "            \n",
    "    assert len(data) == len(sparse_data)        #is the output array the same length as input?\n",
    "    assert sum(sparse_data[0]) != 0             #do these sparse arrays contain anything?\n",
    "    \n",
    "    return np.array(sparse_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "victor = Vectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'dolphin' is represented as feature dimension 13868\n",
      "'the' is represented as feature dimension 45372\n",
      "'coffee' is represented as feature dimension 9677\n"
     ]
    }
   ],
   "source": [
    "index_dict = dict()\n",
    "for i, word in enumerate(classifier.sorted_words):\n",
    "  index_dict[word] = i\n",
    "\n",
    "for word in ['dolphin', 'the', 'coffee']:\n",
    "  if word in classifier.words:\n",
    "    print(\"'%s' is represented as feature dimension %i\" %(word, index_dict[word]))\n",
    "  else:\n",
    "    print(\"'%s' is not in the vocabulary\" % word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = set(classifier.sorted_words)      #I will take it as a given this will be useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "transform() missing 1 required positional argument: 'data'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_raw\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: transform() missing 1 required positional argument: 'data'"
     ]
    }
   ],
   "source": [
    "X = victor.transform(classifier.X_raw)            #output should be bag of words rep of sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part3\n",
    "**Implementation task:** You should implement your versions of the following parts (you can also find this in the slides):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set hyperparameters (these variables are only here for clarity)\n",
    "reguliser_dampening = 0.001   # lambda\n",
    "learning_rate = .1            # gamma\n",
    "\n",
    "# Create the untrained classifier\n",
    "classy = model(loss='hinge', penalty='l2',\n",
    "                      alpha=reguliser_dampening,\n",
    "                      learning_rate='constant', eta0=learning_rate)\n",
    "\n",
    "# Train the classifier\n",
    "model.fit(X, y)\n",
    "\n",
    "# Get the parameter vector\n",
    "omega = np.concatenate([model.intercept_, model.coef_.ravel()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    " \n",
    "plt.figure(figsize=(20, 3))\n",
    "plt.plot(omega[1:])\n",
    "plt.xlabel(\"Value\")\n",
    "plt.xlabel(\"Weights\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert (len(omega)-1) == len(vocabulary)\n",
    "\n",
    "# Sort by absolute value\n",
    "idx = np.argsort(np.abs(omega[1:]))\n",
    "\n",
    "print(\"                Word   Weight  Occurences\")\n",
    "for i in idx[-20:]:   # Pick those with highest 'voting' values\n",
    "  print(\"%20s   %.3f\\t%i \" % (ordered_vocabulary[i], omega[i+1], np.sum([ordered_vocabulary[i] in d for d in X_raw])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part4\n",
    "**Implementation task:** Implement code for printing a sorted table of your sampled hyperparameters. Note, you do not have to reimplement the grid search."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
