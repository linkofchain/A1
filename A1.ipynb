{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tarfile\n",
    "import jdc      #for class handling in jupyter\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part1\n",
    "**Implementation task:** Implement a parser for the dataset. The output should be a list/array of strings (`X_raw`) and a list/array of labels (`y`) encoded as {-1,1}."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!wget -N http://www.cs.cornell.edu/people/pabo/movie-review-data/review_polarity.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_tar():\n",
    "    # open file \n",
    "    review_file = tarfile.open('review_polarity.tar.gz') \n",
    "  \n",
    "    # extracting file \n",
    "    review_file.extractall('.') \n",
    "  \n",
    "    review_file.close() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"_summary_\n",
    "grab data from directory paths and lists\n",
    "\"\"\"\n",
    "def get_data(dir_list, dir_path, review_polarity):\n",
    "  words = set()\n",
    "  reviews = []\n",
    "  sentiment = []\n",
    "  \n",
    "  for file_name in dir_list:\n",
    "      if isinstance(file_name, str):\n",
    "          f = open(os.path.join(dir_path, file_name),'r')\n",
    "          review = f.read()\n",
    "          reviews.append(review)\n",
    "          words = words|set(review.split()) #.strip(\"\") for future comparison\n",
    "          sentiment.append(review_polarity)\n",
    "          f.close()\n",
    "  return np.array(reviews), np.array(sentiment), words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_dict = {} #dictionary of words:index in bag of words vector\n",
    "#read_tar()\n",
    "neg_path = 'txt_sentoken/neg'\n",
    "pos_path = 'txt_sentoken/pos'\n",
    "neg_files = os.listdir(neg_path)\n",
    "pos_files = os.listdir(pos_path)\n",
    "\n",
    "neg_rev, neg_sent, neg_words = get_data(neg_files, neg_path, -1)\n",
    "pos_rev, pos_sent, pos_words = get_data(pos_files, pos_path, 1)\n",
    "#form word bag, X_raw, and prediction sets\n",
    "words = neg_words|pos_words\n",
    "X_raw = np.concatenate((neg_rev,pos_rev),axis=0)\n",
    "y = np.concatenate((neg_sent,pos_sent),axis=0)\n",
    "sorted_words = sorted(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#shuffles both predictions and raw data with same permutation.\n",
    "def shuffle(data, labels):\n",
    "    p = np.random.permutation(len(data))\n",
    "    return data[p], labels[p]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_shuffled, y_shuffled = shuffle(X_raw, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000,)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_shuffled.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dimensions look good! now to move onto implementing BOW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part2\n",
    "\n",
    "**Implementation task:** You should re-implement the feature extraction above. The list/array called `ordered_vocabulary` should contain the words for each feature dimension, and X should contain the BOW binary vectors. Remember to use the same method names as the original sklearn class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"_summary_\n",
    "transform x into a sparse BoW array. implementation part 2\n",
    "\"\"\"\n",
    "class Vectorizer:\n",
    "  def __init__(self, sorted_vocab, tokenizer = 'none'):\n",
    "    self.tokenizer = tokenizer\n",
    "    self.index_dict = dict()\n",
    "    for i, word in enumerate(sorted_vocab):\n",
    "      self.index_dict[word] = i\n",
    "  \n",
    "  def transform(self, data):\n",
    "    sparse_data = []\n",
    "    for review in data:\n",
    "      review_list = review.split()            #replace with tokenizer if needed\n",
    "      sparse_review = np.zeros(len(self.index_dict))\n",
    "      for word in review_list:\n",
    "          sparse_review[self.index_dict[word]] = 1\n",
    "      sparse_data.append(sparse_review)\n",
    "            \n",
    "    assert len(data) == len(sparse_data)        #is the output array the same length as input?\n",
    "    assert sum(sparse_data[0]) != 0             #do these sparse arrays contain anything?\n",
    "    \n",
    "    return np.array(sparse_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "victor = Vectorizer(sorted_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'dolphin' is represented as feature dimension 13868\n",
      "'the' is represented as feature dimension 45372\n",
      "'coffee' is represented as feature dimension 9677\n"
     ]
    }
   ],
   "source": [
    "index_dict = dict()\n",
    "for i, word in enumerate(sorted_words):\n",
    "  index_dict[word] = i\n",
    "\n",
    "for word in ['dolphin', 'the', 'coffee']:\n",
    "  if word in sorted_words:\n",
    "    print(\"'%s' is represented as feature dimension %i\" %(word, index_dict[word]))\n",
    "  else:\n",
    "    print(\"'%s' is not in the vocabulary\" % word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = set(sorted_words)      #I will take it as a given this will be useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = victor.transform(X_shuffled)            #output should be bag of words rep of sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part3\n",
    "**Implementation task:** You should implement your versions of the following parts (you can also find this in the slides):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class model:\n",
    "  def __init__(self, eta0 = 0.1, loss = 'hinge', alpha = .0001, learning_rate = \"constant\", penalty = 'L2', random_state = 'none',\n",
    "               tol = 5):\n",
    "    \n",
    "    self.loss = loss  #hinge loss\n",
    "    self.loss_history = []\n",
    "    self.eta0 = eta0  #starting learning rate\n",
    "    self.alpha = alpha #regularization dampener\n",
    "    self.learning_rate = learning_rate  #do we use constant learning rate or schedule?\n",
    "    self.penalty = penalty #penalty for hinge loss i think?\n",
    "    self.random_state = random_state #random permutation desired for input? maybe not needed\n",
    "    self.tol = tol  #tolerance of learning rate scheduler if i ever get around to implementing that\n",
    "    self.weights = None\n",
    "    \n",
    "  def fit(self, X, y):\n",
    "    if self.weights == None:\n",
    "      self.weights = np.random.rand(self.X_raw.shape[0])\n",
    "    \"\"\"Does the training. X is a matrix with one data point per row, while y is flat.\"\"\"\n",
    "    X = X.ravel()\n",
    "    best_score = self.score(X,y)\n",
    "    self.loss.append(best_score)\n",
    "    early_stop,iter_wo_improve = 20,0\n",
    "    \n",
    "    for i in range(self.n_max_iterations):\n",
    "      self._fit(X,y)\n",
    "      if best_score == self.loss[-1]: #if the new theta isn't better than last, then increment iter_wo_improve.\n",
    "        iter_wo_improve += 1\n",
    "      else:\n",
    "        best_score = self.loss[-1]\n",
    "      if iter_wo_improve >= early_stop: #implement learning rate scheduler here\n",
    "        break\n",
    "      \n",
    "  #gradient descent here?\n",
    "  def _fit(self, X, y): \n",
    "    \"\"\"Internal method that performs one iteration of the training. Should store a loss value in self.loss .\"\"\" \n",
    "    #gradient descent\n",
    "    out = y*X.dot(self.weights)\n",
    "    cond = X*y[:,np.newaxis]   #why is a (1,2000) matrix valid but (,2000) isn't\n",
    "    grad_sum = out[:,np.newaxis]*np.sign(np.maximum(0,1-cond))\n",
    "    \n",
    "    return grad_sum+self.alpha*self.weights\n",
    "    \n",
    "    norm_term = self.alpha*self.weights\n",
    "    hing = []\n",
    "    for i in range(len(X)):\n",
    "      if X*self.weights:\n",
    "        loss_gradient = self.alpha*self.weights\n",
    "    \n",
    "    if self.learning_rate == \"constant\":\n",
    "      new_weights = self.weights-self.eta0*loss_gradient\n",
    "    \n",
    "    #is new param vector better than last one? compare score residual, if better keep, otherwise discard.\n",
    "    old_score = self.loss[-1]\n",
    "    old_theta = self.theta.copy()\n",
    "    self.theta += np.random.normal(size=len(self.theta)) #replace with gradient descent.\n",
    "    new_score = self.score(X,y)\n",
    "    if old_score > new_score:\n",
    "      self.loss.append(new_score)\n",
    "    else:\n",
    "      self.theta = old_theta\n",
    "\n",
    "  #hyperplane model here?\n",
    "  def predict(self, X):\n",
    "    return np.sign(X*self.weights)\n",
    "  #just delete this shit.\n",
    "    signs = []\n",
    "    for review in X:\n",
    "      assert review.shape == self.weights.shape   #if not, did I add dummy values to right place?\n",
    "      proddy = np.dot(review,self.weights)\n",
    "      if proddy < 0:\n",
    "        signs.append(-1)\n",
    "      elif proddy == 0:\n",
    "        signs.append(0)\n",
    "      else:\n",
    "        signs.append(1)\n",
    "        \n",
    "    return np.array(signs)\n",
    "\n",
    "  #hinge loss for base\n",
    "  def score(self, X, y):\n",
    "    #how to define t? if on right side of hyperplane, 1, otherwise -1\n",
    "    assert X.shape[0] == y.shape[0] #to ensure I can do pairwise mult.\n",
    "    if self.loss == 'hinge':\n",
    "      \n",
    "      \n",
    "      np.linalg.multi_dot(X*self.weights)   #should iterate through  X to dot with weights\n",
    "      L2 = self.alpha/2*sum(self.weights**2)\n",
    "      sum_term = np.sum(X*self.weights*y)\n",
    "      return L2+sum(max(0,1-y*self.weights*X))\n",
    "      return max(0,1-t*y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set hyperparameters (these variables are only here for clarity)\n",
    "reguliser_dampening = 0.001   # lambda\n",
    "learning_rate = .001            # gamma\n",
    "\n",
    "# Create the untrained classifier\n",
    "classy = model(loss='hinge', penalty='l2',\n",
    "                      alpha=reguliser_dampening,\n",
    "                      learning_rate='constant', eta0=learning_rate)\n",
    "\n",
    "# Train the classifier\n",
    "#model.fit(X, y_shuffled)\n",
    "\n",
    "# Get the parameter vector\n",
    "#omega = np.concatenate([model.intercept_, model.coef_.ravel()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1]\n",
      " [2]\n",
      " [3]]\n",
      "[ 21.  53. 132.]\n"
     ]
    }
   ],
   "source": [
    "n1 = np.arange(9.0).reshape((3, 3))\n",
    "n2 = np.vstack([1,2,3])\n",
    "#rand_weights = np.random.rand(X_shuffled.shape[0])\n",
    "\n",
    "#print(X_raw.shape, n1.shape, n2.shape, np.column_stack(y).shape)\n",
    "print(n2)\n",
    "print(np.maximum(7,n2*n1*n2.ravel()).sum(axis=1))\n",
    "#print(n1, n2, n1*n2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50920,) (2000, 50920) (2000,)\n"
     ]
    }
   ],
   "source": [
    "#grad desc lab.\n",
    "w8s = np.random.rand(X.shape[1])\n",
    "print(w8s.shape, X.shape, y_shuffled.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = y_shuffled*X.dot(w8s)\n",
    "cond = X*y_shuffled[:,np.newaxis]   #why is a (1,2000) matrix valid but (,2000) isn't\n",
    "grad_sum = out[:,np.newaxis]*np.sign(np.maximum(0,1-cond))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "grad_sum + w8s*.0001\n",
    "w8s += .001*(grad_sum+w8s*.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    " \n",
    "plt.figure(figsize=(20, 3))\n",
    "plt.plot(omega[1:])\n",
    "plt.xlabel(\"Value\")\n",
    "plt.xlabel(\"Weights\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert (len(omega)-1) == len(vocabulary)\n",
    "\n",
    "# Sort by absolute value\n",
    "idx = np.argsort(np.abs(omega[1:]))\n",
    "\n",
    "print(\"                Word   Weight  Occurences\")\n",
    "for i in idx[-20:]:   # Pick those with highest 'voting' values\n",
    "  print(\"%20s   %.3f\\t%i \" % (ordered_vocabulary[i], omega[i+1], np.sum([ordered_vocabulary[i] in d for d in X_raw])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part4\n",
    "**Implementation task:** Implement code for printing a sorted table of your sampled hyperparameters. Note, you do not have to reimplement the grid search."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
