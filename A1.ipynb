{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tarfile\n",
    "import jdc      #for class handling in jupyter\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!wget -N http://www.cs.cornell.edu/people/pabo/movie-review-data/review_polarity.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_tar():\n",
    "    # open file \n",
    "    review_file = tarfile.open('review_polarity.tar.gz') \n",
    "  \n",
    "    # extracting file \n",
    "    review_file.extractall('.') \n",
    "  \n",
    "    review_file.close() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"_summary_\n",
    "grab data from directory paths and lists\n",
    "\"\"\"\n",
    "def get_data(dir_list, dir_path, review_polarity):\n",
    "  words = set()\n",
    "  reviews = []\n",
    "  sentiment = []\n",
    "  \n",
    "  for file_name in dir_list:\n",
    "      if isinstance(file_name, str):\n",
    "          f = open(os.path.join(dir_path, file_name),'r')\n",
    "          review = f.read()\n",
    "          reviews.append(review)\n",
    "          words = words|set(review.split()) #.strip(\"\") for future comparison\n",
    "          sentiment.append(review_polarity)\n",
    "          f.close()\n",
    "  return np.array(reviews), np.array(sentiment), words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_dict = {} #dictionary of words:index in bag of words vector\n",
    "#read_tar()\n",
    "neg_path = 'txt_sentoken/neg'\n",
    "pos_path = 'txt_sentoken/pos'\n",
    "neg_files = os.listdir(neg_path)\n",
    "pos_files = os.listdir(pos_path)\n",
    "\n",
    "neg_rev, neg_sent, neg_words = get_data(neg_files, neg_path, -1)\n",
    "pos_rev, pos_sent, pos_words = get_data(pos_files, pos_path, 1)\n",
    "#form word bag, X_raw, and prediction sets\n",
    "words = neg_words|pos_words\n",
    "X_raw = np.concatenate((neg_rev,pos_rev),axis=0)\n",
    "y = np.concatenate((neg_sent,pos_sent),axis=0)\n",
    "sorted_words = sorted(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#shuffles both predictions and raw data with same permutation.\n",
    "def shuffle(data, labels):\n",
    "    p = np.random.permutation(len(data))\n",
    "    return data[p], labels[p]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_shuffled, y_shuffled = shuffle(X_raw, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000,)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_shuffled.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dimensions look good! now to move onto implementing BOW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"_summary_\n",
    "transform x into a sparse BoW array. implementation part 2\n",
    "\"\"\"\n",
    "class Vectorizer:\n",
    "  def __init__(self, sorted_vocab, tokenizer = 'none'):\n",
    "    self.tokenizer = tokenizer\n",
    "    self.index_dict = dict()\n",
    "    for i, word in enumerate(sorted_vocab):\n",
    "      self.index_dict[word] = i\n",
    "  \n",
    "  def transform(self, data):\n",
    "    sparse_data = []\n",
    "    for review in data:\n",
    "      review_list = review.split()            #replace with tokenizer if needed\n",
    "      sparse_review = np.zeros(len(self.index_dict))\n",
    "      for word in review_list:\n",
    "          sparse_review[self.index_dict[word]] = 1\n",
    "      sparse_data.append(sparse_review)\n",
    "            \n",
    "    assert len(data) == len(sparse_data)        #is the output array the same length as input?\n",
    "    assert sum(sparse_data[0]) != 0             #do these sparse arrays contain anything?\n",
    "    \n",
    "    return np.array(sparse_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "victor = Vectorizer(sorted_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'dolphin' is represented as feature dimension 13868\n",
      "'the' is represented as feature dimension 45372\n",
      "'coffee' is represented as feature dimension 9677\n"
     ]
    }
   ],
   "source": [
    "index_dict = dict()\n",
    "for i, word in enumerate(sorted_words):\n",
    "  index_dict[word] = i\n",
    "\n",
    "for word in ['dolphin', 'the', 'coffee']:\n",
    "  if word in sorted_words:\n",
    "    print(\"'%s' is represented as feature dimension %i\" %(word, index_dict[word]))\n",
    "  else:\n",
    "    print(\"'%s' is not in the vocabulary\" % word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = set(sorted_words)      #I will take it as a given this will be useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = victor.transform(X_shuffled)            #output should be bag of words rep of sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part3\n",
    "**Implementation task:** You should implement your versions of the following parts (you can also find this in the slides):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class model:\n",
    "  def __init__(self, eta0 = 0.001, loss = 'hinge', alpha = .0001, learning_rate = \"constant\", penalty = 'L2', random_state = 'none',\n",
    "               tol = 5):\n",
    "    \n",
    "    self.loss = loss  #hinge loss default\n",
    "    self.loss_history = []\n",
    "    self.lr, self.eta0 = eta0, eta0  #starting learning rate\n",
    "    self.alpha = alpha #regularization dampener\n",
    "    self.learning_rate = learning_rate  #do we use constant learning rate or schedule?\n",
    "    self.penalty = penalty #penalty for hinge loss i think?\n",
    "    self.random_state = random_state #random permutation desired for input? maybe not needed\n",
    "    self.tol = tol  #tolerance of learning rate scheduler if i ever get around to implementing that\n",
    "    self.weights = np.random.rand(self.X_raw.shape[0])\n",
    "    self.weight_history = [].append(self.weights)\n",
    "    \n",
    "  def fit(self, X, y):\n",
    "    \"\"\"Does the training. X is a matrix with one data point per row, while y is flat.\"\"\"\n",
    "    best_score = self.score(X,y)\n",
    "    self.loss.append(best_score)\n",
    "    iter_wo_improve = 0\n",
    "    \n",
    "    for i in range(self.n_max_iterations):\n",
    "      self._fit(X,y)\n",
    "      if best_score-.001 < self.loss[-1]: #if the new theta isn't better than last, then increment iter_wo_improve.\n",
    "        iter_wo_improve += 1\n",
    "      else:\n",
    "        best_score = self.loss[-1]\n",
    "        iter_wo_improve = 0\n",
    "      if iter_wo_improve >= self.tol: #stop criterion\n",
    "        break\n",
    "      \n",
    "  \"\"\"gradient descent\"\"\"\n",
    "  def _fit(self, X, y): \n",
    "    #gradient descent\n",
    "    cond = X.dot(self.weights)*y\n",
    "    out = -X*y[:,np.newaxis]   #why is a (1,2000) matrix valid but (,2000) isn't\n",
    "    #if cond is < 1, then 1-cond > 0, sign(1-cond) will be 1 when cond < 1.\n",
    "    #multiply out by max between 0 and sign(1-cond) to get out when cond < 1\n",
    "    grad_sum = np.sum(out*np.sign(np.maximum(0,1-cond[:,np.newaxis])),axis=0)\n",
    "    \n",
    "    grad = self.alpha*self.weights + grad_sum\n",
    "    \n",
    "    self.weights -= self.lr*grad\n",
    "    \n",
    "    self.loss_history.append(self.score(X,y))\n",
    "    self.weight_history.append(self.weights)\n",
    "    \n",
    "  \"\"\"#hyperplane model here?\"\"\"\n",
    "  def predict(self, X):\n",
    "    return np.sign(np.sum(X*self.weights))\n",
    "\n",
    "  \"\"\"#hinge loss for basemodel\"\"\"\n",
    "  def score(self, X, y):\n",
    "    if self.loss == 'hinge':\n",
    "      term1 = self.alpha/2*np.linalg.norm(self.weights)**2\n",
    "      term2 = np.sum(np.maximum(0,1-y.dot(self.weights*X)))\n",
    "      \n",
    "      return term1+term2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "reguliser_dampening = 0.001   # lambda\n",
    "learning_rate = .001          # gamma\n",
    "\n",
    "# Create the untrained classifier\n",
    "classy = model(loss='hinge', penalty='l2',\n",
    "                      alpha=reguliser_dampening,\n",
    "                      learning_rate='constant', eta0=learning_rate)\n",
    "\n",
    "# Train the classifier\n",
    "classy.fit(X, y_shuffled)\n",
    "\n",
    "# Get the parameter vector\n",
    "omega = classy.weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    " \n",
    "plt.figure(figsize=(20, 3))\n",
    "plt.plot(omega[1:])\n",
    "plt.xlabel(\"Value\")\n",
    "plt.xlabel(\"Weights\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert (len(omega)-1) == len(vocabulary)\n",
    "\n",
    "# Sort by absolute value\n",
    "idx = np.argsort(np.abs(omega[1:]))\n",
    "\n",
    "print(\"                Word   Weight  Occurences\")\n",
    "for i in idx[-20:]:   # Pick those with highest 'voting' values\n",
    "  print(\"%20s   %.3f\\t%i \" % (ordered_vocabulary[i], omega[i+1], np.sum([ordered_vocabulary[i] in d for d in X_raw])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part4\n",
    "**Implementation task:** Implement code for printing a sorted table of your sampled hyperparameters. Note, you do not have to reimplement the grid search."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
